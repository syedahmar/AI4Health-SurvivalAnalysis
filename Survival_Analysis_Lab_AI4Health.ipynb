{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf9752b",
   "metadata": {},
   "source": [
    "# Survival Analysis Lab: From Classical to Machine Learning Methods in Heart Attack Patients\n",
    "\n",
    "In this lab, you'll work with real-world survival data from the **Worcester Heart Attack Study (WHAS500)** — a cohort of 500 patients hospitalised with acute myocardial infarction.\n",
    "\n",
    "It's good practice to read the official documentation before working with any dataset. For WHAS500, refer to:  \n",
    "🔗 https://scikit-survival.readthedocs.io/en/stable/api/generated/sksurv.datasets.load_whas500.html\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 What You'll Learn\n",
    "\n",
    "This lab brings the theory of survival analysis to life. You'll apply key survival analysis methods to a clinical dataset and reflect on their strengths, limitations, and interpretations.\n",
    "\n",
    "We will walk through:\n",
    "\n",
    "### 🧭 Lab Sections\n",
    "\n",
    "**1. Load and Inspect Data**  \n",
    "Get familiar with the WHAS500 dataset and prepare variables for modelling.\n",
    "\n",
    "**2. Kaplan-Meier Estimation**  \n",
    "Estimate and visualise survival curves; compare survival across subgroups.\n",
    "\n",
    "**3. Cox Proportional Hazards Model**  \n",
    "Fit a classical Cox model, interpret hazard ratios, and visualise adjusted curves.\n",
    "\n",
    "**4. Evaluating Performance: The Concordance Index**  \n",
    "Quantify model discrimination ability using the concordance index (C-index).\n",
    "\n",
    "**5. Random Survival Forests**  \n",
    "Model nonlinear survival patterns and visualise predicted survival curves by group.\n",
    "\n",
    "**6. DeepSurv: Deep Learning for Survival Analysis**  \n",
    "Use a neural network to learn complex risk relationships and compare model performance.\n",
    "\n",
    "**7. Summary and Reflections**  \n",
    "Compare all models, interpret results in context, and reflect on trade-offs between interpretability and flexibility.\n",
    "\n",
    "---\n",
    "\n",
    "This lab is designed to complement the associated chapter from my upcoming book, reinforcing survival concepts with realistic, hands-on modelling in a healthcare context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd309d-208a-48ca-9784-f733250ccf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9f015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-survival lifelines pandas matplotlib seaborn --quiet\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sksurv.datasets import load_whas500\n",
    "from sksurv.util import Surv\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.nonparametric import kaplan_meier_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55902988",
   "metadata": {},
   "source": [
    "## Section 1: Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de90963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "X_struct, y_struct = load_whas500()\n",
    "\n",
    "# Convert feature array to DataFrame\n",
    "X = pd.DataFrame(X_struct)\n",
    "\n",
    "# Convert target array to a DataFrame for inspection \n",
    "y_df = pd.DataFrame(y_struct)\n",
    "\n",
    "# Combine for inspection\n",
    "X['event'] = y_struct['fstat']\n",
    "X['time'] = y_struct['lenfol']\n",
    "\n",
    "# Prepare target for modeling\n",
    "y = Surv.from_dataframe('event', 'time', X)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34496e7-6cd5-452c-8377-fb83523c3363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasgui import show\n",
    "gui = show(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e074d-bc02-4bbf-be7c-03018a0f5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types\n",
    "print(X.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47629e2e",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f50030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of patient ages to understand their distribution\n",
    "\n",
    "# Create histogram with 20 bins\n",
    "X['age'].hist(bins=20)\n",
    "\n",
    "# Add title and axis labels for clarity\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f941f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of patients by gender\n",
    "\n",
    "# Map gender codes to labels for readability\n",
    "gender_labels = {'0': 'Female', '1': 'Male'}\n",
    "X['gender'].map(gender_labels).value_counts().plot(kind='bar')\n",
    "\n",
    "# Add a title and rotate x-axis labels for readability\n",
    "plt.title('Gender Distribution')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b4db43-1695-4100-93a7-4fdc5ad3a2ec",
   "metadata": {},
   "source": [
    "### Question: How does survival time vary with congestive heart failure (CHF) status?\n",
    "💡Use boxplot and summary statistics to compare follow-up time (lenfol) and event rates between patients with and without CHF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab3fca-4fdd-43a6-bcbd-c14774d14af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e02c9c-123d-426d-ac22-bb1a9284044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 1. View how many patients had congestive heart failure (CHF)\n",
    "print(\"CHF value counts:\")\n",
    "print(X['chf'].value_counts())\n",
    "\n",
    "# # 2. Boxplot: Compare survival time (lenfol / 'time') between CHF groups\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x='chf', y='time', data=X)\n",
    "plt.title('Survival Time by CHF Status')\n",
    "plt.xlabel('Congestive Heart Failure (0 = No, 1 = Yes)')\n",
    "plt.ylabel('Follow-up Time (days)')\n",
    "plt.show()\n",
    "\n",
    "# # 3. Compare proportion of deaths by CHF group\n",
    "death_rates = X.groupby('chf')['event'].mean()\n",
    "print(\"\\nProportion of deaths in each CHF group:\")\n",
    "print(death_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbcce84-ecf6-4df3-98ee-5362f257763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💡 Uncomment below to reveal the solution\n",
    "\n",
    "# # 1. View how many patients had congestive heart failure (CHF)\n",
    "# print(\"CHF value counts:\")\n",
    "# print(X['chf'].value_counts())\n",
    "\n",
    "# # 2. Boxplot: Compare survival time (lenfol / 'time') between CHF groups\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# sns.boxplot(x='chf', y='time', data=X)\n",
    "# plt.title('Survival Time by CHF Status')\n",
    "# plt.xlabel('Congestive Heart Failure (0 = No, 1 = Yes)')\n",
    "# plt.ylabel('Follow-up Time (days)')\n",
    "# plt.show()\n",
    "\n",
    "# # 3. Compare proportion of deaths by CHF group\n",
    "# death_rates = X.groupby('chf')['event'].mean()\n",
    "# print(\"\\nProportion of deaths in each CHF group:\")\n",
    "# print(death_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf448a0",
   "metadata": {},
   "source": [
    "## Section 2: Kaplan-Meier Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71dab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "time, survival_prob = kaplan_meier_estimator(X['event'], X['time'])\n",
    "plt.step(time, survival_prob, where='post')\n",
    "plt.title('Kaplan-Meier Curve')\n",
    "plt.xlabel('Time (days)')\n",
    "plt.ylabel('Survival probability')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f6a31-6450-420b-8603-9b29d1b1f845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💡 Plot overall survival curve\n",
    "time, survival_prob = kaplan_meier_estimator(y['event'], y['time'])\n",
    "plt.step(time, survival_prob, where='post', label='Overall')\n",
    "\n",
    "# 💡 Stratify by CHF status (0 = No CHF, 1 = CHF)\n",
    "for chf_value in ['0', '1']:\n",
    "    mask = X['chf'] == chf_value\n",
    "    time_chf, surv_chf = kaplan_meier_estimator(y['event'][mask], y['time'][mask])\n",
    "    label = f\"CHF = {chf_value}\"\n",
    "    plt.step(time_chf, surv_chf, where=\"post\", label=label)\n",
    "\n",
    "# Plot formatting\n",
    "plt.title(\"Kaplan-Meier Survival Curves by CHF Status\")\n",
    "plt.xlabel(\"Time (days)\")\n",
    "plt.ylabel(\"Survival Probability\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58865c2-be60-44cf-a676-767bbe0136a4",
   "metadata": {},
   "source": [
    "🔧 **Try This**  \n",
    "Use the plot above to answer the following:\n",
    "\n",
    "- Which group appears to have the **worst survival**? Why?\n",
    "- Does the **gap between curves** stay constant over time, or change?\n",
    "- What might this suggest about the **proportional hazards assumption** for CHF?\n",
    "\n",
    "*Hint: You could follow this up by fitting a Cox model that includes CHF as a covariate.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6911eaab-e861-43cf-949a-3ba9b4c6a46d",
   "metadata": {},
   "source": [
    "### Question  \n",
    "Based on the Kaplan-Meier curves stratified by CHF status, which interpretation is most appropriate?\n",
    "\n",
    "A. Patients with CHF have consistently lower survival probabilities across the follow-up period.  \n",
    "B. CHF patients initially have worse survival, but then outperform non-CHF patients later.  \n",
    "C. CHF status only affects survival during the first 30 days.  \n",
    "D. All patients, regardless of CHF, follow the same survival trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97682f1",
   "metadata": {},
   "source": [
    "## Section 3: Cox Proportional Hazards Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c7bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.util import Surv\n",
    "\n",
    "# Prepare data for Cox model\n",
    "cox_vars = ['age', 'gender', 'hr', 'chf']\n",
    "X_model = X[cox_vars]\n",
    "y_model = Surv.from_dataframe(\"event\", \"time\", X)\n",
    "\n",
    "# Fit Cox model\n",
    "cox_model = CoxPHSurvivalAnalysis()\n",
    "cox_model.fit(X_model, y_model)\n",
    "\n",
    "# Display hazard ratios\n",
    "hazard_ratios = pd.Series(cox_model.coef_, index=cox_vars).apply(np.exp)\n",
    "hazard_ratios.name = \"Hazard Ratio\"\n",
    "display(hazard_ratios)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97313ec-3f48-456e-9485-5be43490c6c2",
   "metadata": {},
   "source": [
    "🔧 **Try This**  \n",
    "- Which variable has the **strongest association** with survival?\n",
    "- What does the hazard ratio for **CHF** tell you?\n",
    "- Is the effect of CHF larger or smaller than age or heart rate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985cf79-3a3d-4d5e-bb5c-96e055984f70",
   "metadata": {},
   "source": [
    "❓ **Quiz Question**  \n",
    "Your Cox model shows a hazard ratio of 2.1 for CHF. What does this mean?\n",
    "\n",
    "A. CHF patients have a 2.1× higher **probability** of dying at any point.  \n",
    "B. CHF patients have a 2.1× **instantaneous risk** of death, adjusting for other variables.  \n",
    "C. CHF causes death in 2.1× more patients.  \n",
    "D. CHF patients survive twice as long as others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21696e7c-2ded-4f49-84a0-7f3a38d9c91a",
   "metadata": {},
   "source": [
    "## Section 4: Evaluating Survival Model Performance: The Concordance Index (C-index)\n",
    "\n",
    "To assess how well a survival model is performing, we need a way to judge not just the predicted survival probabilities, but how well the model **ranks** patients by risk.\n",
    "\n",
    "The **concordance index (C-index)** is the most widely used metric for this in survival analysis.\n",
    "\n",
    "### 🧠 What does the C-index measure?\n",
    "\n",
    "It answers this question:\n",
    "\n",
    "> \"Among all pairs of patients where we know which one died earlier, how often did the model correctly assign a higher risk to the one who died first?\"\n",
    "\n",
    "In simpler terms, it tells us how well the model can **discriminate** between higher-risk and lower-risk patients.\n",
    "\n",
    "- **C-index = 1.0** → perfect ranking\n",
    "- **C-index = 0.5** → no better than random guessing\n",
    "- **C-index < 0.5** → worse than random (usually a red flag)\n",
    "\n",
    "✅ It works even when not all patients have experienced the event, and it properly handles censored data.\n",
    "\n",
    "We’ll use it now to evaluate the Cox model, and later apply the same metric to other models like Random Survival Forests and DeepSurv — making comparison easy and fair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47caa40a-9d3a-4b15-ba95-c37eda06a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import concordance_index_censored\n",
    "# let us first convert the data types of gender and chf so that the functions can handle it (converting from category to integer)\n",
    "def encode_categoricals(df):\n",
    "    return df.apply(lambda col: col.cat.codes if col.dtype.name == 'category' else col)\n",
    "X_model_v2 = encode_categoricals(X_model.copy())\n",
    "# Get predicted risk scores (higher = higher risk)\n",
    "risk_scores = cox_model.predict(X_model_v2)\n",
    "\n",
    "# Evaluate with C-index\n",
    "c_index, _, _, _, _ = concordance_index_censored(\n",
    "    y_model['event'], y_model['time'], risk_scores\n",
    ")\n",
    "\n",
    "print(f\"Concordance Index (Cox model): {c_index:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7c6b76-f4d7-4820-91de-0710c796297c",
   "metadata": {},
   "source": [
    "❗ **Note on Risk Score Direction in Cox Models**\n",
    "\n",
    "When you use `cox_model.predict(X)` from `sksurv`, it returns **log hazard scores** — not survival probabilities.\n",
    "\n",
    "That means:\n",
    "\n",
    "- **Higher scores** = **higher risk**\n",
    "- **Lower scores** = **lower risk**\n",
    "- So there's **no need to flip the sign** when using these scores for evaluation\n",
    "\n",
    "In contrast, other models like Random Survival Forests often predict **survival probabilities** — where **lower probabilities** indicate higher risk.  \n",
    "In those cases, we do apply a minus sign (`-`) to convert survival to risk before calculating the C-index.\n",
    "\n",
    "✅ Bottom line:  \n",
    "If you're using Cox model predictions directly, don’t apply a negative sign — they already rank patients by risk in the correct direction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f28f0d-7a57-44d6-9f14-019a64f9be45",
   "metadata": {},
   "source": [
    "### Publication-Ready Forest Plots with `lifelines`\n",
    "\n",
    "While `sksurv` is great for modeling, it doesn’t natively return confidence intervals or standard errors needed for polished visualisations.\n",
    "\n",
    "To create **publication-ready forest plots**, we’ll use the `lifelines` package. It produces clear, interpretable plots with:\n",
    "- Hazard ratios on a log scale\n",
    "- 95% confidence intervals\n",
    "- A reference line at HR = 1 (no effect)\n",
    "\n",
    "This is especially helpful when communicating results to non-technical stakeholders or preparing figures for clinical papers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec99cd-2c7b-4162-a4c1-51b1234db469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare dataset\n",
    "df_lifelines = X[['time', 'event', 'age', 'gender', 'hr', 'chf']].copy()\n",
    "\n",
    "# Fit Cox model\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(df_lifelines, duration_col='time', event_col='event')\n",
    "\n",
    "# Plot with confidence intervals\n",
    "cph.plot(hazard_ratios=True)\n",
    "plt.title(\"Forest Plot: Cox Model Hazard Ratios with 95% CI\")\n",
    "plt.axvline(x=1.0, color='red', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf3e1af",
   "metadata": {},
   "source": [
    "## Section 5: Random Survival Forests (RSF)\n",
    "\n",
    "Random Survival Forests (RSFs) are an extension of random forests for time-to-event data.\n",
    "\n",
    "They are especially useful when:\n",
    "- The relationship between predictors and survival is **nonlinear**\n",
    "- Covariates may **interact** in complex ways\n",
    "- The **proportional hazards assumption** doesn’t hold\n",
    "\n",
    "RSFs learn survival patterns by growing many decision trees that split patients based on survival time and censoring status.\n",
    "\n",
    "Unlike the Cox model, RSFs do not assume a specific form for the hazard — they are fully data-driven and model-free.\n",
    "\n",
    "Let’s build and train an RSF using our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395452c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.util import Surv\n",
    "\n",
    "# Prepare input data\n",
    "X_model = X[['age', 'gender', 'hr', 'chf']]\n",
    "y_model = Surv.from_dataframe(\"event\", \"time\", X)\n",
    "\n",
    "# Fit Random Survival Forest\n",
    "rsf = RandomSurvivalForest(n_estimators=100,\n",
    "                           min_samples_split=10,\n",
    "                           min_samples_leaf=15,\n",
    "                           max_features=\"sqrt\",\n",
    "                           n_jobs=-1,\n",
    "                           random_state=42)\n",
    "rsf.fit(X_model, y_model)\n",
    "\n",
    "print(\"Random Survival Forest model trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c5e0fa-da40-4059-95f6-cb7fd9813217",
   "metadata": {},
   "source": [
    "### Concordance Index for Random Survival Forests\n",
    "\n",
    "To fairly compare different survival models, we’ll use the **same evaluation metric** — the concordance index (C-index).\n",
    "\n",
    "While the Cox model returns log-hazard scores, the RSF predicts **survival functions**.\n",
    "\n",
    "To compute the C-index for RSF, we:\n",
    "\n",
    "1. Predict each patient’s **survival probability** at a fixed time point (e.g. 1 year).\n",
    "2. Convert that to a **risk score**: lower survival = higher risk.\n",
    "3. Feed those risk scores into the concordance index function.\n",
    "\n",
    "Let’s calculate it below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ee424-60d1-4800-b6ac-c2a607ef47b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "# Predict survival function for each patient\n",
    "surv_funcs_rsf = rsf.predict_survival_function(X_model)\n",
    "\n",
    "# Get survival probability at 365 days for each patient\n",
    "surv_1yr = np.array([fn(365) for fn in surv_funcs_rsf])\n",
    "\n",
    "# Convert to risk scores: lower survival = higher risk\n",
    "rsf_risk_scores = -surv_1yr\n",
    "\n",
    "# Compute concordance index\n",
    "c_index_rsf, _, _, _, _ = concordance_index_censored(\n",
    "    y_model[\"event\"],\n",
    "    y_model[\"time\"],\n",
    "    rsf_risk_scores\n",
    ")\n",
    "\n",
    "print(f\"Concordance Index (Random Survival Forest): {c_index_rsf:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0428a653-51d8-4aca-8176-03bbad83906b",
   "metadata": {},
   "source": [
    "### Feature Importance in RSF\n",
    "\n",
    "Understanding **which variables influence survival most** is a common goal in both classical and machine learning models.\n",
    "\n",
    "In `scikit-survival`, the `.feature_importances_` function is often **not implemented** for Random Survival Forests.  \n",
    "Even if the attribute appears available, trying to use it may raise an error or produce unreliable results.\n",
    "\n",
    "✅ For reliable results, we recommend using **permutation importance**, which works across models and gives a more robust estimate of which features matter most.\n",
    "\n",
    "This approach helps us understand:\n",
    "- Which covariates most affect the model’s survival predictions\n",
    "- How the RSF prioritises features based on real-world data patterns\n",
    "\n",
    "🧠 **How Permutation Importance Works**\n",
    "\n",
    "Permutation importance helps us understand which features a model actually *relies on* to make predictions.\n",
    "\n",
    "Here’s how it works:\n",
    "\n",
    "1. **Train the model as usual** on the full dataset.\n",
    "\n",
    "2. Then, for each feature (e.g. age, heart rate, CHF status):\n",
    "   - **Randomly shuffle** the values of that feature across all patients.\n",
    "   - This breaks any relationship between that feature and the outcome.\n",
    "   - Run the model again on this shuffled version.\n",
    "\n",
    "3. **Compare performance**:\n",
    "   - If the model’s accuracy or predictions get *much worse*, the feature was important.\n",
    "   - If there's *little change*, the model wasn’t using that feature much.\n",
    "\n",
    "This process is repeated multiple times to get a stable estimate.\n",
    "\n",
    "✅ It works with **any model**, including Random Survival Forests and deep learning — making it a powerful, model-agnostic way to measure variable importance.\n",
    "\n",
    "💡 In survival analysis, we can evaluate how shuffling a feature affects predictions like **1-year survival probability** or **concordance index**.\n",
    "\n",
    "Let’s compute and visualise permutation-based feature importance next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c788b-4351-4f2c-9000-b68cd5965aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: Before you shuffle any feature, evaluate the model’s average predicted 1-year survival across all patients:\n",
    "\n",
    "# Baseline: predicted survival at 1 year for original data\n",
    "surv_funcs = rsf.predict_survival_function(X_model)\n",
    "baseline_survival = np.array([fn(365) for fn in surv_funcs])\n",
    "baseline_score = np.mean(baseline_survival)\n",
    "print(baseline_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272948b1-37b7-4371-9dae-5ae32befe7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2: Define a new scoring function that compares to the baseline\n",
    "# Scoring function: negative average survival probability at 365 days\n",
    "def rsf_score_absolute_diff(model, X, baseline_score):\n",
    "    surv_funcs = model.predict_survival_function(X)\n",
    "    new_surv = np.array([fn(365) for fn in surv_funcs])\n",
    "    new_score = np.mean(new_surv)\n",
    "    return abs(new_score - baseline_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861c153-fc7b-4a34-816c-6fac9e73e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compute permutation importance manually\n",
    "#Since this approach doesn’t fit directly into sklearn's permutation_importance, we’ll implement a manual version:\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Repeat shuffling to get average importance\n",
    "n_repeats = 10\n",
    "importances = []\n",
    "\n",
    "for col in X_model.columns:\n",
    "    scores = []\n",
    "    for _ in range(n_repeats):\n",
    "        X_temp = X_model.copy()\n",
    "        X_temp[col] = np.random.permutation(X_temp[col].values)\n",
    "        score = rsf_score_absolute_diff(rsf, X_temp, baseline_score)\n",
    "        scores.append(score)\n",
    "    importances.append(np.mean(scores))\n",
    "\n",
    "# Convert to series for plotting\n",
    "importances_series = pd.Series(importances, index=X_model.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b1b382-10e7-4871-9770-1c634b9ca5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Plot\n",
    "importances_series.sort_values().plot.barh()\n",
    "plt.title(\"Permutation Importance – Absolute Deviation (1-Year Survival)\")\n",
    "plt.xlabel(\"Average Change in Predicted Survival\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5a1fe-9f0f-4b53-b8f3-23407cc2b123",
   "metadata": {},
   "source": [
    "### Interpreting Permutation Importance (Absolute Change in Survival)\n",
    "\n",
    "To understand which features our Random Survival Forest model truly depends on, we use **permutation importance**.\n",
    "\n",
    "Instead of just checking if survival predictions go down, we check how much they **change**, in any direction.\n",
    "\n",
    "Here’s what we did:\n",
    "\n",
    "1. **Computed baseline predictions**:\n",
    "   - We predicted each patient’s chance of surviving 1 year.\n",
    "   - Then we averaged those predictions across the whole cohort.\n",
    "\n",
    "2. **Shuffled each feature** one at a time:\n",
    "   - This breaks its relationship with the outcome.\n",
    "   - We recomputed the 1-year survival predictions.\n",
    "\n",
    "3. **Measured how much the average survival prediction changed**:\n",
    "   - If a feature is important, shuffling it will lead to a big shift in predictions.\n",
    "   - If the model wasn’t using the feature, predictions stay about the same.\n",
    "\n",
    "We repeated this 10 times per feature and averaged the result.\n",
    "\n",
    "📊 The resulting plot shows how **sensitive** the model’s survival predictions are to each feature — a powerful way to reveal what’s truly driving risk.\n",
    "### 💡 What makes Permutation Importance so Useful?\n",
    "\n",
    "- It works with **any kind of model**, not just forests.\n",
    "- It gives an intuitive sense of **real model reliance**.\n",
    "- It doesn’t assume anything about the shape or direction of effects.\n",
    "\n",
    "---\n",
    "\n",
    "📌 **Bottom Line**:  \n",
    "Permutation importance tells you **how much the model depends on each variable** by messing with them and seeing what breaks.\n",
    "\n",
    "It’s simple. Powerful. And great for survival models, especially when built-in importance scores don’t work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c57bb7-3b6a-48b2-b765-f2bc439aeb15",
   "metadata": {},
   "source": [
    "🔧 **Try This: Interpreting the Permutation Plot**\n",
    "\n",
    "1. Why might **age** have the largest impact on 1-year survival predictions in this model?\n",
    "2. Are you surprised that **CHF** appears less important than **gender**? What could explain that?\n",
    "3. Heart rate (hr) seems to have the smallest impact. \n",
    "   - Could this be due to its actual clinical role?\n",
    "   - Or might it reflect something about how this model handles continuous variables?\n",
    "4. Would you expect these rankings to change if:\n",
    "   - You used a Cox model instead?\n",
    "   - You used longer-term survival (e.g. 2-year survival) instead of 1-year?\n",
    "\n",
    "💡 *Bonus*: Think of a variable not in this dataset that could improve survival prediction. How might you test its importance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4942f9cb-f95f-4dfc-8ddf-50073b4ba08d",
   "metadata": {},
   "source": [
    "## Section 6: DeepSurv: Deep Learning for Survival Analysis\n",
    "\n",
    "So far, we’ve explored two classical approaches:\n",
    "- **Cox proportional hazards**, which assumes a linear log-risk relationship\n",
    "- **Random Survival Forests**, which model nonlinearities but are tree-based\n",
    "\n",
    "What if we want to combine the flexibility of nonlinear modelling with the strength of survival frameworks like Cox?\n",
    "\n",
    "### 🎯 Enter DeepSurv\n",
    "\n",
    "**DeepSurv** is a deep learning model that generalises the Cox proportional hazards model by replacing the linear predictor with a neural network.\n",
    "\n",
    "- It **learns complex, nonlinear relationships** between features and survival risk\n",
    "- Uses the **same partial likelihood loss** as Cox — so it still models **relative risk**\n",
    "- Supports **censoring**, just like classical survival models\n",
    "- Can be extended to handle time-varying covariates or competing risks\n",
    "\n",
    "### ⚙️ How It Works\n",
    "\n",
    "- Inputs: Patient features (e.g. age, heart rate, comorbidities)\n",
    "- A feedforward neural network transforms those features\n",
    "- Output: A **risk score** (just like Cox, but now nonlinear)\n",
    "- Trained using **negative partial likelihood loss** from Cox\n",
    "\n",
    "### 📈 Why Use DeepSurv?\n",
    "\n",
    "- You don’t have to pre-specify interactions or transformations\n",
    "- Learns directly from data\n",
    "- Ideal when you suspect complex patterns (e.g. nonlinearity, thresholds, covariate interactions)\n",
    "\n",
    "We’ll now train a DeepSurv model using the `pycox` library and evaluate it using the same **concordance index** as before — allowing fair comparison with Cox and RSF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cda6d1-53bb-4271-a55b-ed74cb633639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Install required packages\n",
    "#!pip install pycox torchtuples\n",
    "\n",
    "# STEP 2: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchtuples as tt\n",
    "from pycox.models import CoxPH\n",
    "from pycox.evaluation import EvalSurv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sksurv.datasets import load_whas500\n",
    "from sksurv.util import Surv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# STEP 3: Load dataset\n",
    "X_raw, y_struct = load_whas500()\n",
    "X = pd.DataFrame(X_raw)\n",
    "y_df = pd.DataFrame(y_struct)\n",
    "y = Surv.from_dataframe(\"fstat\", \"lenfol\", y_df)\n",
    "\n",
    "# Prepare features\n",
    "X_model = X[[\"age\", \"gender\", \"hr\", \"chf\"]].copy()\n",
    "X_model[\"gender\"] = X_model[\"gender\"].cat.codes\n",
    "X_model[\"chf\"] = X_model[\"chf\"].cat.codes\n",
    "\n",
    "# STEP 4: Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_model)\n",
    "\n",
    "# STEP 5: Format target for pycox\n",
    "y_df = pd.DataFrame.from_records(y)\n",
    "y_duration = y_df[\"lenfol\"].values\n",
    "y_event = y_df[\"fstat\"].values\n",
    "\n",
    "# STEP 6: Train-test split\n",
    "X_train, X_test, y_train_dur, y_test_dur, y_train_ev, y_test_ev = train_test_split(\n",
    "    X_scaled, y_duration, y_event, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# STEP 7: Prepare target as tuple of (durations, events)\n",
    "y_train = (y_train_dur, y_train_ev)\n",
    "y_test = (y_test_dur, y_test_ev)\n",
    "\n",
    "# STEP 8: Define and train the DeepSurv model\n",
    "net = tt.practical.MLPVanilla(\n",
    "    in_features=X_train.shape[1],\n",
    "    num_nodes=[32, 32],\n",
    "    out_features=1,\n",
    "    activation=torch.nn.ReLU,\n",
    "    dropout=0.1\n",
    ")\n",
    "model = CoxPH(net, tt.optim.Adam)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32') # this ensures that the data and model are using float32\n",
    "model.fit(X_train, y_train, batch_size=256, epochs=100, verbose=False)\n",
    "model.compute_baseline_hazards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6261145-a725-4f1a-8451-3be8bdc0a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "# STEP 1: Predict survival functions for the test set\n",
    "model.compute_baseline_hazards()\n",
    "surv = model.predict_surv_df(X_test)\n",
    "\n",
    "# STEP 2: Create an evaluator object\n",
    "ev = EvalSurv(\n",
    "    surv,                   # Predicted survival functions (DataFrame: time × patients)\n",
    "    y_test[0],              # Durations (time to event or censoring)\n",
    "    y_test[1],              # Events (1 = event, 0 = censored)\n",
    "    censor_surv='km'        # How to handle censoring (Kaplan-Meier by default)\n",
    ")\n",
    "\n",
    "# STEP 3: Calculate concordance index\n",
    "c_index = ev.concordance_td('antolini')\n",
    "\n",
    "print(f\"Concordance Index (DeepSurv): {c_index:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f793a1-603c-4834-a42e-e4a91a9d918a",
   "metadata": {},
   "source": [
    "🧪 **Optional: Improve DeepSurv with Early Stopping**\n",
    "\n",
    "Training deep models on small clinical datasets can lead to overfitting. One way to mitigate this is to use **early stopping**, which monitors validation loss and stops training when performance stops improving.\n",
    "\n",
    "In `pycox`, you can add early stopping like this:\n",
    "\n",
    "```python\n",
    "from torchtuples.callbacks import EarlyStopping\n",
    "\n",
    "callbacks = [EarlyStopping(patience=10)]\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=64,\n",
    "    epochs=300,\n",
    "    val_data=(X_test, y_test),\n",
    "    callbacks=callbacks\n",
    ")\n",
    "model.compute_baseline_hazards()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f574e4f7-d7ba-41aa-aa6c-d69e9cba4718",
   "metadata": {},
   "source": [
    "## Section 7: Summary and Reflections\n",
    "\n",
    "---\n",
    "### Summary\n",
    "In this lab, you explored multiple approaches to modelling time-to-event outcomes:\n",
    "\n",
    "| Model                  | Strengths                                      | Considerations                         |\n",
    "|------------------------|-----------------------------------------------|----------------------------------------|\n",
    "| Cox Proportional Hazards | Interpretable, widely used                    | Assumes linear effects, proportional hazards |\n",
    "| Random Survival Forest  | Handles nonlinearity, robust to outliers      | No hazard ratios; harder to interpret  |\n",
    "| DeepSurv                | Learns complex patterns from data              | Needs tuning, data hungry              |\n",
    "\n",
    "You also:\n",
    "- Calculated **Kaplan-Meier curves** to visualise survival patterns\n",
    "- Compared models using the **concordance index (C-index)**\n",
    "- Learned to interpret **survival curves and hazard ratios**\n",
    "- Visualised patient-level predictions and stratified risk patterns\n",
    "\n",
    "Each model comes with trade-offs between interpretability, flexibility, and data requirements. There is no single “best” model — the right choice depends on your **data**, **goal**, and **audience**.\n",
    "\n",
    "---\n",
    "### Reflect and Apply\n",
    "\n",
    "1. **Clinical Use Case**\n",
    "   - Which model would you recommend if interpretability is key?\n",
    "   - Which would you choose for a purely predictive task?\n",
    "\n",
    "2. **Data Fit**\n",
    "   - What assumptions does the Cox model make?\n",
    "   - When might RSF or DeepSurv be preferable?\n",
    "\n",
    "3. **Model Limitations**\n",
    "   - What risks do we face when applying these models blindly?\n",
    "   - How do censoring and competing risks affect model evaluation?\n",
    "\n",
    "4. **Visual Thinking**\n",
    "   - Which survival curves did you find most helpful to interpret?\n",
    "   - How would you present these results to a clinical or policy audience?\n",
    "\n",
    "---\n",
    "### Key Takeaways\n",
    "\n",
    "- **Survival analysis** is essential for modelling time-to-event outcomes — from hospital readmissions to treatment failure.\n",
    "- **Interpretability** matters — especially in healthcare. Know what your model tells you, and what it doesn’t.\n",
    "- **Evaluation** must account for censoring. Concordance index is a great start, but always ask: *“What does this number really mean for decisions?”*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871c449f-c602-4322-ab68-40526a0359a1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
